# LSTM Text Generator - Self-Learning Chat AI ğŸ§ 

A **true text generation AI** built from scratch that generates responses character-by-character using LSTM neural networks!

## ğŸŒŸ Features

- **Character-Level LSTM** - Generates text from scratch (not canned responses!)
- **24/7 Background Training** - Continuously learns from conversations
- **From-Scratch Implementation** - Pure Python/NumPy, no TensorFlow/PyTorch
- **Self-Learning** - Improves with every conversation
- **Model Persistence** - Saves progress automatically
- **Web Interface** - Beautiful chat UI

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install numpy flask flask-cors
```

### 2. Start the LSTM Server

```bash
python server.py
```

Server starts on `http://localhost:5000`

### 3. Open Chat Interface

Open `index.html` in your browser!

## ğŸ§  How It Works

### LSTM Architecture

```
Input (Character) â†’ LSTM Cell â†’ Output (Next Character)
                       â†“
            [Forget Gate | Input Gate | Output Gate]
                       â†“
               Hidden State â†’ Next Prediction
```

### Character-Level Generation

Instead of word-level (like simple chatbots), this AI:
1. Encodes text character-by-character
2. Learns patterns in the character sequences
3. Generates NEW text one character at a time
4. Can create responses it's never seen before!

### 24/7 Training Loop

```python
Background Thread:
  while True:
    - Check for new conversations
    - Retrain on updated data
    - Save model checkpoints
    - Continuously improve
```

## ğŸ“ Project Structure

```
ai-chat/
â”œâ”€â”€ lstm_generator.py        # LSTM implementation from scratch
â”œâ”€â”€ continuous_trainer.py    # 24/7 training system
â”œâ”€â”€ server.py                # Flask API server
â”œâ”€â”€ app.js                   # Frontend JavaScript
â”œâ”€â”€ index.html              # Chat UI
â”œâ”€â”€ style.css               # Styling
â”œâ”€â”€ lstm_model.pkl          # Saved model (auto-generated)
â””â”€â”€ conversations.txt       # Training data (auto-generated)
```

## ğŸ¯ API Endpoints

- `POST /chat` - Send message, get AI response
- `GET /stats` - View training statistics
- `POST /generate` - Custom text generation
- `GET /history` - Conversation history
- `POST /save` - Manual model save

## ğŸ”§ Technical Details

### LSTM Implementation

- **Input Size**: Character vocabulary size
- **Hidden Size**: 128 neurons
- **Sequence Length**: 25 characters
- **Gates**: Forget, Input, Output, Candidate
- **Training**: Backpropagation Through Time (BPTT)

### Training Process

1. One-hot encode characters
2. Forward pass through LSTM
3. Calculate loss (cross-entropy)
4. Backward pass (gradients)
5. Update weights
6. Repeat continuously!

## ğŸ“Š Stats Dashboard

View real-time stats:
- Training status (Active/Idle)
- Total training iterations
- Current loss
- Vocabulary size
- Conversations learned
- Training queue size

## ğŸ’¡ Tips

### First Run
- Model trains ~5 minutes on startup
- Starts with basic conversation knowledge
- Improves dramatically after 10+ conversations

### Better Responses
- Talk more! AI learns from every chat
- Use complete sentences
- Vary your questions
- After 50+ conversations, quality improves significantly

### Model Management
- Auto-saves every 5 minutes
- Delete `lstm_model.pkl` to retrain from scratch
- `conversations.txt` stores all training data

## ğŸ¨ What Makes This Special?

### vs. Traditional Chatbots:
âŒ Traditional: Match pattern â†’ Return canned response  
âœ… LSTM: Learn patterns â†’ **Generate NEW text**

### vs. Pre-trained Models:
âŒ GPT/BERT: Use pre-trained weights  
âœ… This: **Built from scratch**, learns YOUR data

### Real Text Generation:
```
User: "Hello!"
Bot: Doesn't just return "Hi there!"
Bot: GENERATES: "Hello! I'm learning..." 
     (created character-by-character!)
```

## ğŸ”¬ Advanced Usage

### Custom Generation

```python
from lstm_generator import CharLSTM

lstm = CharLSTM()
lstm.load('lstm_model.pkl')

# Generate text
text = lstm.generate(
    seed_text="Hello",
    length=100,
    temperature=0.8  # Higher = more creative
)
```

### Manual Training

```python
from continuous_trainer import ContinuousTrainer

trainer = ContinuousTrainer()
trainer.add_conversation("user input", "ai response")
# Automatically trains in background!
```

## ğŸ“ˆ Monitoring Progress

Watch the console:
```
ğŸ“š Added conversation to training queue
ğŸ“– Training on 3 new conversations...
  Iteration 10/50, Loss: 0.2314
ğŸ’¾ Auto-saving model...
ğŸ’ª Background training - Iteration 1000, Loss: 0.1847
```

## âš ï¸ Limitations

- **Not as smart as GPT** (needs millions more parameters!)
- **Requires training time** (gets better over days)
- **CPU-intensive** (best on decent machines)
- **Small vocabulary initially** (expands with use)

## ğŸ“ Learning Resources

This implements:
- **LSTM networks** (Long Short-Term Memory)
- **Backpropagation Through Time** (BPTT)
- **Character-level language modeling**
- **Continuous learning systems**

Perfect for learning how real AI text generation works!

---

**Built with â¤ï¸ using Pure Python & NumPy**  
*No TensorFlow, No PyTorch - Just Math!*
